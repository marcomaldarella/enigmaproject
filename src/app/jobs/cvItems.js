export const cvItems = [
  {
    name: "Research Scientist: Multi-Modal Modeling",
    description: `We are seeking exceptional research scientists to pioneer the development of foundation models that bridge artificial and biological intelligence. You will lead the development of large-scale transformer-based architectures that integrate diverse neural data streamsâ€”from visual stimuli to high-dimensional neuronal recordings and behavioral measurements. This position offers a unique opportunity to push the boundaries of self-supervised learning and multi-task objectives, creating models that not only predict neural responses but reveal fundamental principles of biological computation. The ideal candidate will have extensive experience developing multimodal foundation models and interest in pioneering the application of these techniques for decoding the neural basis of intelligence.

      Role & Responsibilities:
      - Design novel transformer-based architectures for integrating continuous visual, neural, and behavioral time series data
      - Develop self-supervised learning approaches and multi-task objectives for training foundation models of the brain
      - Pioneer new methods for modeling the relationship between sensory inputs and neural activity across the visual hierarchy
      - Lead research in scaling model architectures to process and integrate massive neurophysiological datasets
      - Guide technical strategy for model evaluation, validation, and interpretation
      - Advance the field through publications and presentations at top machine learning and computational neuroscience venues

      Key Qualifications:
      - Ph.D. in Computer Science, Machine Learning, Computational Neuroscience, or a related field, plus 2+ years post-Ph.D. research experience
      - At least 2+ years of practical experience in training, fine-tuning, and using multi-modal deep learning models
      - Strong publication record in top-tier machine learning conferences and journals, particularly in areas related to multi-modal modeling
      - Strong programming skills in Python and deep learning frameworks
      - Demonstrated ability to lead research projects and mentor others
      - Ability to work effectively in a collaborative, multidisciplinary environment

      Preferred Qualifications:
      - Background in theoretical neuroscience or computational neuroscience
      - Experience in processing and analyzing large-scale, high-dimensional data of different sources
      - Experience with cloud computing platforms (e.g., AWS, GCP, Azure) and their machine learning services
      - Familiarity with big data and MLOps platforms (e.g. MLflow, Weights & Biases)
      - Familiarity with training, fine-tuning, and quantization of LLMs or multimodal models using common techniques and frameworks (LoRA, PEFT, AWQ, GPTQ, or similar)
      - Experience with large-scale distributed model training frameworks (e.g., Ray, DeepSpeed, HF Accelerate, FSDP)

      What We Offer:
      - A rich environment in which to pursue fundamental research questions in AI and neuroscience
      - A dynamic team of engineers and scientists in a project dedicated to one mission, rooted in academia but inspired by science in industry
      - Access to unique datasets spanning artificial and biological neural networks
      - State-of-the-art computing infrastructure
      - Competitive salary and benefits package
      - Collaborative environment at the intersection of multiple disciplines
      - Location at Stanford University with access to its world-class research community
      - Strong mentoring in career development

      Application:
      Please send your CV and one-page interest statement to: recruiting@enigmaproject.ai`
  },
  {
    name: "Research Engineer: Multi-Modal Modeling",
    description: `We are seeking exceptional engineers to build and scale the next generation of brain foundation models. You will develop robust infrastructure for training large-scale transformer architectures that process continuous, multi-dimensional neural and behavioral time series data. This role focuses on implementing efficient training pipelines, optimizing model architectures, and solving the unique engineering challenges of working with massive neurophysiological datasets. The ideal candidate will have extensive experience implementing and scaling multimodal foundation models and a drive to tackle the computational challenges of modeling biological intelligence. This position offers an opportunity to build the technical foundation for a new understanding of how the brain processes information.

    Role & Responsibilities:
    - Implement and optimize the latest machine learning algorithms/models to train multimodal foundation models on neural data
    - Develop and maintain scalable, efficient, and reproducible machine-learning pipelines
    - Conduct large-scale ML experiments, using the latest MLOps platforms
    - Run large-scale distributed model training on high-performance computing clusters or cloud platforms
    - Collaborate with machine learning researchers, data scientists, and systems engineers to ensure seamless integration of models and infrastructure
    - Monitor and optimize model performance, resource utilization, and cost-effectiveness
    - Stay up-to-date with the latest advancements in machine learning tools, frameworks, and methodologies

    Key Qualifications:
    - Master's or Ph.D. in Computer Science, Machine Learning, or a related field
    - 2-3 years of practical experience in implementing and optimizing machine learning algorithms with distributed training using common libraries (e.g., Ray, DeepSpeed, HF Accelerate, FSDP)
    - Strong programming skills in Python, with expertise in machine learning frameworks like TensorFlow or PyTorch
    - Experience with orchestration platforms
    - Experience with cloud computing platforms (e.g., AWS, GCP, Azure) and their machine learning services
    - Familiarity with MLOps platforms (e.g., MLflow, Weights & Biases)
    - Strong understanding of software engineering best practices, including version control, testing, and documentation

    Preferred Qualifications:
    - Familiarity with training, fine-tuning, and quantization of LLMs or multimodal models using common techniques and frameworks (LoRA, PEFT, AWQ, GPTQ, or similar)
    - Familiarity with modern big data tools and pipelines such as Apache Spark, Arrow, Airflow, Delta Lake, or similar
    - Experience with AutoML and Neural Architecture Search (NAS) techniques
    - Contributions to open-source machine learning projects or libraries

    What We Offer:
    - Work on a collaborative and uniquely positioned project spanning several disciplines, from neuroscience to artificial intelligence and engineering
    - Competitive salary and benefits
    - Strong mentoring in career development

    Application:
    Please send your CV and one-page interest statement to: recruiting@enigmaproject.ai`
  },
  {
    name: "Research Scientist: Interpretability, Theory, & Analysis",
    description: `We are seeking exceptional research scientists who can advance the theoretical foundations of interpretability research while developing novel methods for understanding computational principles in both artificial and biological neural networks. This position will drive forward our understanding of how large-scale neural systems process and represent information, with the unique opportunity to apply and develop interpretability techniques across both artificial and biological systems. The role combines cutting-edge research in mechanistic interpretability with the opportunity to impact our understanding of both artificial and biological intelligence.

  Role & Responsibilities:
  - Lead development of automated methods for interpreting large-scale neural networks and biological data
  - Design algorithms for discovering computational principles and circuits in neural systems
  - Advance techniques for feature visualization, geometric analysis, and manifold learning in high-dimensional neural data
  - Develop causal intervention methods to map information flow in neural networks
  - Create tools for automated hypothesis generation and testing in neural systems
  - Collaborate with neuroscientists to validate interpretability findings in biological systems
  - Guide technical strategy for scaling interpretability methods to massive datasets

  Key Qualifications:
  - Ph.D. in Computer Science, Mathematics, Neuroscience, or related field plus 2+ years post-Ph.D. research experience
  - Strong publication record in machine learning, particularly in areas related to model interpretability
  - Deep understanding of mechanistic interpretability literature and methods
  - Expertise in analyzing and interpreting deep neural networks
  - Experience with automated scientific discovery systems or agentic AI
  - Strong programming skills with experience in modern ML frameworks
  - Demonstrated ability to lead research projects and mentor others
  - Excellent written and verbal communication skills

  Preferred Qualifications:
  - Experience developing novel interpretability methods
  - Background in theoretical neuroscience or computational neuroscience
  - Knowledge of differential geometry and its applications to neural representations
  - Familiarity with large-scale machine learning systems
  - Track record of open-source contributions to interpretability tools
  - Experience with large language models or multimodal architectures
  - History of successful research collaborations across disciplines

  Research Areas of Interest:
  - Novel methods for mechanistic interpretability at scale
  - Geometric approaches to understanding neural representations
  - Development of AI scientists for automated hypothesis generation and testing
  - Techniques for discovering and validating computational circuits
  - Comparative analyses between artificial and biological neural networks
  - Causal intervention methods for understanding network computation
  - Mathematical frameworks for neural information processing

  What We Offer:
  - An environment in which to pursue fundamental research questions in AI and neuroscience interpretability
  - Access to unique datasets spanning artificial and biological neural networks
  - State-of-the-art computing infrastructure
  - Competitive salary and benefits package
  - Collaborative environment at the intersection of multiple disciplines
  - Location at Stanford University with access to its world-class research community

  Application:
  Please send your CV and a one-page statement of interest to: recruiting@enigmaproject.ai`
  },
  {
    name: "Research Engineer: Interpretability, Theory, & Analysis",
    description: `We are seeking exceptional engineers to develop and deploy scalable pipelines for analyzing and interpreting foundation models of the brain, helping us understand how the brain represents and processes information. This position will focus on applying and scaling state-of-the-art neural analyses and interpretability techniques to uncover meaningful structures and circuits within our brain foundation models. The role combines rigorous engineering practices with cutting-edge research in model interpretability, working at the intersection of neuroscience and artificial intelligence.

  Role & Responsibilities:
  - Design and implement scalable pipelines for automated interpretability analyses of brain foundation models
  - Develop infrastructure for running massive-scale in silico experiments on digital twins
  - Build tools for automated circuit discovery and geometric/topological analysis of neural manifolds
  - Create efficient, reproducible analysis workflows for processing high-dimensional neural data
  - Engineer systems for automated hypothesis generation and testing
  - Implement and scale feature visualization and manifold learning techniques
  - Maintain distributed computing infrastructure for parallel interpretability analyses
  - Develop interactive visualization tools for exploring neural representations

  Key Qualifications:
  - Master's degree in Computer Science or related field with 2+ years of relevant industry experience, OR Bachelor's degree with 4+ years of relevant industry experience
  - Strong understanding of mechanistic interpretability techniques and research literature
  - Expertise in implementing and scaling ML analysis pipelines
  - Experience with high-performance computing and distributed systems
  - Proficiency in Python and deep learning frameworks (i.e., PyTorch)
  - Experience with distributed computing and high-performance computing clusters
  - Strong software engineering practices including version control, testing, and documentation
  - Familiarity with visualization tools and techniques for high-dimensional data

  Preferred Qualifications:
  - Experience with feature visualization techniques (e.g., activation maximization, attribution methods)
  - Knowledge of geometric methods for analyzing neural population activity
  - Familiarity with circuit discovery techniques in neural networks
  - Experience with large-scale data processing frameworks
  - Background in neuroscience or computational neuroscience
  - Contributions to open-source ML or interpretability tools
  - Experience with ML experiment tracking platforms (W&B, MLflow)

  What We Offer:
  - Opportunity to work on fundamental questions in AI interpretability and neuroscience
  - Collaborative environment bridging academic research and engineering excellence
  - Access to state-of-the-art computing resources and unique neural datasets
  - Competitive salary and benefits
  - Career development and mentoring
  - Location at Stanford University with access to its vibrant research community

  Application:
  Please send your CV and a one-page statement of interest to: recruiting@enigmaproject.ai`
  }
];
